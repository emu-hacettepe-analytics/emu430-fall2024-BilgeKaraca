[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Our Course Project",
    "section": "",
    "text": "I‚Äôm honored to be a member of the ANALYTICA project team.\nBelow, you‚Äôll find a brief summary of our project. To access a detailed project description, please go to https://emu-hacettepe-analytics.github.io/emu430-fall2024-team-analytica/\nSummary\nTraffic accidents pose a significant challenge for cities, affecting safety, traffic flow, and public well-being. To better understand the dynamics of traffic accidents in ƒ∞zmir, we utilized the data set titled ‚Äúƒ∞zmir Metropolitan Municipality Defective, Accident Vehicle Data‚Äù, published on the ƒ∞zmir Metropolitan Municipality Open Data Portal. This data set provides valuable information such as the date, type, time, and destination of traffic accidents that occurred within the metropolitan area.\nThe primary aim of this project is to analyze and interpret the data to uncover patterns, identify critical problem areas, and offer insights that can contribute to improved traffic management and accident prevention strategies.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Bilge‚Äôs Analytics Lab",
    "section": "",
    "text": "Hello! My name is Bilge Karaca.\nThis is my personal webpage.\nPlease stay tuned to follow my works on data analytics, blog posts, and more.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/assignment-2.html",
    "href": "assignments/assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Code\n# combining 2 URL's into a single vector\n\nurl_1 &lt;- \"https://m.imdb.com/search/title/?title_type=feature&release_date=2010-01-01,2023-12-31&num_votes=2500,&country_of_origin=TR&count=250\"\n\nurl_2 &lt;- \"https://m.imdb.com/search/title/?title_type=feature&release_date=,2009-12-31&num_votes=2500,&country_of_origin=TR&count=250\"\n\ncombine_vector &lt;- c(url_1, url_2)\n\n# creating empty vectors\n\ntitles &lt;- c()\nyears &lt;- c()\ndurations &lt;- c()\nratings &lt;- c()\nvotes &lt;- c()\n\n# Scrapping the data\n\nfor (url in combine_vector) {\n  page &lt;- read_html(url)\n  \n  # Extract movie names\n  \n  title_names &lt;- page %&gt;% html_nodes('.ipc-title__text') %&gt;% html_text()\n  title_names &lt;- tail(head(title_names, -1), -1)\n  title_names &lt;- str_split(title_names, \" \", n=2)\n  title_names &lt;- unlist(lapply(title_names, function(x) {x[2]}))\n  \n  \n  # Extract year\n  year &lt;- page %&gt;% html_nodes('.sc-300a8231-7:nth-child(1)') %&gt;% html_text() %&gt;% substr(1, 4) %&gt;% as.numeric()\n  \n  \n  # Extract rating\n  rating &lt;- page %&gt;%\n    html_nodes('.ipc-rating-star--rating') %&gt;%\n    html_text() %&gt;%\n    substr(1, 3) %&gt;%\n    as.numeric()\n  \n  \n  # Extract number of votes\n  vote &lt;- page %&gt;% html_nodes(\".ipc-rating-star--voteCount\") %&gt;% html_text() %&gt;% parse_number()\n  vote &lt;- gsub(\"\\\\(|\\\\)| \", \"\", vote)\n  vote &lt;- as.numeric(vote)\n  \n  \n  # Extract duration\n  duration &lt;- page %&gt;% html_nodes('.sc-300a8231-7:nth-child(2)') %&gt;% html_text()\n  \n  \n  # Extract hour part, if present\n  hour &lt;- str_extract(duration, \"\\\\d+h\") %&gt;%\n    str_replace(\"h\", \"\") %&gt;%\n    as.numeric()\n  \n  \n  # If hour is NA, set it to 0\n  hour[is.na(hour)] &lt;- 0\n  \n  \n  # Extract minute part, if present\n  minute &lt;- str_extract(duration, \"\\\\d+m\") %&gt;%\n    str_replace(\"m\", \"\") %&gt;%\n    as.numeric()\n  \n  \n  # If minute is NA, set it to 0\n  minute[is.na(minute)] &lt;- 0\n  \n  \n  # Calculate total duration\n  total_duration &lt;- (hour * 60) + minute\n  \n  \n  # Append data to vectors\n  titles &lt;- append(titles, title_names)\n  years &lt;- append(years, year)\n  ratings &lt;- append(ratings, rating)\n  votes &lt;- append(votes, vote)\n  durations &lt;- append(durations, total_duration)\n}\n\n\n# Create a data frame from the scraped data\nimdb_data &lt;- data.frame(Title = titles, Year = years, Duration = durations, Rating = ratings, Votes = votes)\n\n\n# Print the first few rows\nprint(head(imdb_data))\n\n\n                     Title Year Duration Rating Votes\n1        Kuru Otlar √úst√ºne 2023      197    7.7    16\n2 Yedinci Kogustaki Mucize 2019      132    8.2    58\n3                   Baskin 2015       97    5.8    13\n4               Kis Uykusu 2014      196    8.0    57\n5        Cebimdeki Yabanci 2018       95    6.8    10\n6                     Ayla 2017      125    8.2    45\n\n\n\nAs seen here, our data set, which we scrapped from the IMDB website, contains the columns Title, Year, Duration, Rating and Votes respectively. Votes values ‚Äã‚Äãhere represent thousands. (16 =&gt; 16K, 4.1 =&gt; 4.1K)\nDue to the problem that arose during the scrapping process, the dataset consists of 50 movies, 25 in the first URL (between 2009 and 2023) and the first 25 in the second URL (before 2009).\nSince the data set does not include all movies, the comments made are based on only the 50 movies selected here for all Turkish movies.",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello, my name is Bilge Karaca. I am a senior Industrial Engineering student at Hacettepe University - Ankara. You can find further information about me below."
  },
  {
    "objectID": "about.html#employements",
    "href": "about.html#employements",
    "title": "About Me",
    "section": "Employements",
    "text": "Employements\n\nFirm xxx, position xx, year xxx\nFirm yyy, position yyy, year yyy"
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About Me",
    "section": "Internships",
    "text": "Internships\n\nMAN T√ºrkiye A.≈û. , Production Intern , 24/06/2024 - 22/07/2024\nGDZ Elektrik Daƒüƒ±tƒ±m A.≈û. , Data Analytics Intern , 10/07/2023 - 04/09/2023"
  },
  {
    "objectID": "assignments/assignment-1.html",
    "href": "assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "My first assignment has two parts.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "My Assignments",
    "section": "",
    "text": "On this page, I showcase the assignment I conducted for the term Fall 2024 EMU430 Data Analytics course.\nPlease use left menu to navigate through my assignments.\nThe most recent update to this page was made on January 3, 2025\n\n\n\n Back to top",
    "crumbs": [
      "My Assignments"
    ]
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "This page is under construction.\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/assignment-1.html#b-poll_us_election_2016-dataset",
    "href": "assignments/assignment-1.html#b-poll_us_election_2016-dataset",
    "title": "Assignment 1",
    "section": "(b) poll_us_election_2016 dataset",
    "text": "(b) poll_us_election_2016 dataset\nIn part b, I completed the tasks given on the dataset named ‚Äúpoll_us_election_2016‚Äù.\n\nFirst, I imported the dataset named polls_us_election_2016 in the dslabs package and wanted to see its general content.\n\n\n\nCode\n# importing the dataset \n\nlibrary(dslabs)\n\ndata(polls_us_election_2016)\n\n# general structre of the dataset\n\nstr(polls_us_election_2016)\n\n\n'data.frame':   4208 obs. of  15 variables:\n $ state           : Factor w/ 57 levels \"Alabama\",\"Alaska\",..: 50 50 50 50 50 50 50 50 37 50 ...\n $ startdate       : Date, format: \"2016-11-03\" \"2016-11-01\" ...\n $ enddate         : Date, format: \"2016-11-06\" \"2016-11-07\" ...\n $ pollster        : Factor w/ 196 levels \"ABC News/Washington Post\",..: 1 63 81 194 65 55 18 113 195 76 ...\n $ grade           : Factor w/ 10 levels \"D\",\"C-\",\"C\",\"C+\",..: 10 6 8 6 5 9 8 8 NA 8 ...\n $ samplesize      : int  2220 26574 2195 3677 16639 1295 1426 1282 8439 1107 ...\n $ population      : chr  \"lv\" \"lv\" \"lv\" \"lv\" ...\n $ rawpoll_clinton : num  47 38 42 45 47 ...\n $ rawpoll_trump   : num  43 35.7 39 41 43 ...\n $ rawpoll_johnson : num  4 5.46 6 5 3 3 5 6 6 7.1 ...\n $ rawpoll_mcmullin: num  NA NA NA NA NA NA NA NA NA NA ...\n $ adjpoll_clinton : num  45.2 43.3 42 45.7 46.8 ...\n $ adjpoll_trump   : num  41.7 41.2 38.8 40.9 42.3 ...\n $ adjpoll_johnson : num  4.63 5.18 6.84 6.07 3.73 ...\n $ adjpoll_mcmullin: num  NA NA NA NA NA NA NA NA NA NA ...\n\n\nAs seen here, the data set contains 15 variables/columns and 4208 rows. The columns contain numeric, character and factor data types.\n\nThen I displayed the first 10 lines.\n\n\n\nCode\n# displaying the first 10 rows \n\n(polls_us_election_2016[1:10,])\n\n\n        state  startdate    enddate\n1        U.S. 2016-11-03 2016-11-06\n2        U.S. 2016-11-01 2016-11-07\n3        U.S. 2016-11-02 2016-11-06\n4        U.S. 2016-11-04 2016-11-07\n5        U.S. 2016-11-03 2016-11-06\n6        U.S. 2016-11-03 2016-11-06\n7        U.S. 2016-11-02 2016-11-06\n8        U.S. 2016-11-03 2016-11-05\n9  New Mexico 2016-11-06 2016-11-06\n10       U.S. 2016-11-04 2016-11-07\n                                                     pollster grade samplesize\n1                                    ABC News/Washington Post    A+       2220\n2                                     Google Consumer Surveys     B      26574\n3                                                       Ipsos    A-       2195\n4                                                      YouGov     B       3677\n5                                            Gravis Marketing    B-      16639\n6  Fox News/Anderson Robbins Research/Shaw & Company Research     A       1295\n7                                     CBS News/New York Times    A-       1426\n8                                NBC News/Wall Street Journal    A-       1282\n9                                                    Zia Poll  &lt;NA&gt;       8439\n10                                                   IBD/TIPP    A-       1107\n   population rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin\n1          lv           47.00         43.00            4.00               NA\n2          lv           38.03         35.69            5.46               NA\n3          lv           42.00         39.00            6.00               NA\n4          lv           45.00         41.00            5.00               NA\n5          rv           47.00         43.00            3.00               NA\n6          lv           48.00         44.00            3.00               NA\n7          lv           45.00         41.00            5.00               NA\n8          lv           44.00         40.00            6.00               NA\n9          lv           46.00         44.00            6.00               NA\n10         lv           41.20         42.70            7.10               NA\n   adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin\n1         45.20163      41.72430        4.626221               NA\n2         43.34557      41.21439        5.175792               NA\n3         42.02638      38.81620        6.844734               NA\n4         45.65676      40.92004        6.069454               NA\n5         46.84089      42.33184        3.726098               NA\n6         49.02208      43.95631        3.057876               NA\n7         45.11649      40.92722        4.341786               NA\n8         43.58576      40.77325        5.365788               NA\n9         44.82594      41.59978        7.870127               NA\n10        42.92745      42.23545        6.316175               NA\n\n\n\nThen I found the total number of NA values ‚Äã‚Äãin the entire dataset.\n\n\n\nCode\n# calculating the total number of NA values \n\nsum(is.na(polls_us_election_2016))\n\n\n[1] 11604\n\n\n\nThen I assigned it as a new variable to preserve the original dataset. I continued my subsequent work on this dataset, which I named new_data.\n\n\n\nCode\nnew_data &lt;- polls_us_election_2016\n\n\n\nThen I found which columns in this data set were numeric, which were character, and which were factor. I assigned the relevant indexes of each as variables.\n\n\n\nCode\n# assigning indexes to variables according to data types\n\nnumeric_columns &lt;- which(sapply(new_data, is.numeric))\n\ncharacter_columns &lt;- which(sapply(new_data, is.character))\n\nfactor_columns &lt;- which(sapply(new_data, is.factor))\n\nprint(c(\"numeric: \", numeric_columns ,\"character: \", character_columns, \"Factor: \", factor_columns))\n\n\n                       samplesize  rawpoll_clinton    rawpoll_trump \n     \"numeric: \"              \"6\"              \"8\"              \"9\" \n rawpoll_johnson rawpoll_mcmullin  adjpoll_clinton    adjpoll_trump \n            \"10\"             \"11\"             \"12\"             \"13\" \n adjpoll_johnson adjpoll_mcmullin                        population \n            \"14\"             \"15\"    \"character: \"              \"7\" \n                            state         pollster            grade \n      \"Factor: \"              \"1\"              \"4\"              \"5\" \n\n\n\nThen, I replaced the NA values ‚Äã‚Äãin the columns with numeric data type with my birth year, 2002. I replaced the NA values ‚Äã‚Äãin the columns with data types character and factor with my name Bilge. In order to make changes to the factor type, I first had to change its type to character. After assigning the Bilge name, I changed the data type back to factor.\n\n\n\nCode\n# replacing NA values in numeric columns with 2002\n\nfor (i in numeric_columns) {\n  \n  new_data[is.na(new_data[, i]), i] &lt;- 2002\n  \n}\n\n# replacing NA values in character columns with Bilge\n\nfor (i in character_columns) {\n  \n  new_data[is.na(new_data[, i]), i] &lt;- \"Bilge\"\n  \n}\n\n# replacing NA values in factor columns with Bilge\n\nfor (i in factor_columns) {\n  \n  new_data[, i] &lt;- as.character(new_data[, i])  \n  new_data[is.na(new_data[, i]), i] &lt;- \"Bilge\"\n  new_data[, i] &lt;- as.factor(new_data[, i]) \n}\n\n\n\nThen I displayed the first 10 rows of this newly created dataset.\n\n\n\nCode\n# displaying first 10 row\n\nnew_data[1:10,]\n\n\n        state  startdate    enddate\n1        U.S. 2016-11-03 2016-11-06\n2        U.S. 2016-11-01 2016-11-07\n3        U.S. 2016-11-02 2016-11-06\n4        U.S. 2016-11-04 2016-11-07\n5        U.S. 2016-11-03 2016-11-06\n6        U.S. 2016-11-03 2016-11-06\n7        U.S. 2016-11-02 2016-11-06\n8        U.S. 2016-11-03 2016-11-05\n9  New Mexico 2016-11-06 2016-11-06\n10       U.S. 2016-11-04 2016-11-07\n                                                     pollster grade samplesize\n1                                    ABC News/Washington Post    A+       2220\n2                                     Google Consumer Surveys     B      26574\n3                                                       Ipsos    A-       2195\n4                                                      YouGov     B       3677\n5                                            Gravis Marketing    B-      16639\n6  Fox News/Anderson Robbins Research/Shaw & Company Research     A       1295\n7                                     CBS News/New York Times    A-       1426\n8                                NBC News/Wall Street Journal    A-       1282\n9                                                    Zia Poll Bilge       8439\n10                                                   IBD/TIPP    A-       1107\n   population rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin\n1          lv           47.00         43.00            4.00             2002\n2          lv           38.03         35.69            5.46             2002\n3          lv           42.00         39.00            6.00             2002\n4          lv           45.00         41.00            5.00             2002\n5          rv           47.00         43.00            3.00             2002\n6          lv           48.00         44.00            3.00             2002\n7          lv           45.00         41.00            5.00             2002\n8          lv           44.00         40.00            6.00             2002\n9          lv           46.00         44.00            6.00             2002\n10         lv           41.20         42.70            7.10             2002\n   adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin\n1         45.20163      41.72430        4.626221             2002\n2         43.34557      41.21439        5.175792             2002\n3         42.02638      38.81620        6.844734             2002\n4         45.65676      40.92004        6.069454             2002\n5         46.84089      42.33184        3.726098             2002\n6         49.02208      43.95631        3.057876             2002\n7         45.11649      40.92722        4.341786             2002\n8         43.58576      40.77325        5.365788             2002\n9         44.82594      41.59978        7.870127             2002\n10        42.92745      42.23545        6.316175             2002\n\n\n\nFinally, I calculated how many NA values ‚Äã‚Äãwere in the newly created dataset.\n\n\n\nCode\n# calculating the total number of NA values\n\nsum(is.na(new_data))\n\n\n[1] 0\n\n\nI used AI in 2 different places in this assignment.\n\nFirst, I wrote the following prompt to ChatGPT to find out which columns are numeric:\n\nIf the data type of the columns is numeric, I want to assign the indexes of those columns to a variable called numeric_columns and the answer is:\n\n\nCode\nnumeric_columns &lt;- which(sapply(new_data, is.numeric))\n\n\nI also applied this structure for character and factor ones.\n\nSecondly, I could not change the NA values ‚Äã‚Äãin the factor data type. I asked ChatGPT for help on this issue:\n\nI get an error when changing the values ‚Äã‚Äãin the factor data type. How can I solve this? According to answer, I should first convert it to the character data type and then back to the factor data type:\n\n\nCode\nfor (i in factor_columns) {\n  \n  new_data[, i] &lt;- as.character(new_data[, i])  \n  new_data[is.na(new_data[, i]), i] &lt;- \"Bilge\"\n  new_data[, i] &lt;- as.factor(new_data[, i]) \n}",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#a-summary",
    "href": "assignments/assignment-1.html#a-summary",
    "title": "Assignment 1",
    "section": "(a) summary",
    "text": "(a) summary",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#a-summary-of-the-video",
    "href": "assignments/assignment-1.html#a-summary-of-the-video",
    "title": "Assignment 1",
    "section": "(a) summary of the video",
    "text": "(a) summary of the video\nIn part A, I watched the video ‚ÄúVeri Bilimi ve End√ºstri M√ºhendisliƒüi √úzerine Sohbetler - Baykal Hafƒ±zoƒülu & Erdi Da≈üdemir‚Äù and extracted the brief summary. I have listed the conclusions I made below:\n\nThe world of Operational Research / Analytics can be examined under 4 basic categories. These are:\n\n\nDescriptive Analytics: It covers defining data and understanding the details thoroughly. It is the easiest category in terms of complexity but low in terms of value. Data mining, time series analysis, data visualization are among the methods used.\nDiagnostic Analytics: It aims to diagnose problems. Hypothesis testing, clustering, regression are among the methods used.\nPredictive Analytics: It aims to predict what will happen in the future. Simulation, clustering, regression, machine learning are among the methods used.\nPrescriptive Analytics: It aims to make action suggestions. Although it is the category with the highest complexity, its value is also high. Optimization, heuristics, math modeling are among the methods used.\n\n\nAnalytical solutions can be divided into Operational (several times a day), Tactical (once or twice a month), Strategical (once in a few years) according to their frequency of use.\nAll projects should start with a clear and concise problem definition. At this stage, KPIs, success criteria should be determined and descriptive analyses should be performed.\nThe model produced at the model deployment stage should be suitable for the user and the sector. It should be applicable in real life and easy to use. The platform used and early prototyping are critical. Communication with the end user and the user interface are of critical importance. Even if the model is developed correctly, if it does not meet the user‚Äôs requests, the model will not work.\nAt the last stage, it should be stated how much the KPIs specified at the beginning were developed using this model. In addition, it is always better for the solution to be easily explained compared to complex models.\n\nAfterwards, I prepared two questions about this video, one open-ended and one multiple-choice:\n\nWhat is the most critical concept in the model deployment process?\nAnswer: The user interface is the most important element at this stage. The model must fully comply with the end user‚Äôs requests and needs so that the user can adapt to the model.\nWhich of the following is not one of the categories in which we classify analytical solutions based on frequency of use?\n\n\n\nTactical\nOperational\nStrategical\nExecutional\n\nAnswer: D",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html#scrapping-imdb-dataset",
    "href": "assignments/assignment-2.html#scrapping-imdb-dataset",
    "title": "Assignment 2",
    "section": "",
    "text": "Code\n# combining 2 URL's into a single vector\n\nurl_1 &lt;- \"https://m.imdb.com/search/title/?title_type=feature&release_date=2010-01-01,2023-12-31&num_votes=2500,&country_of_origin=TR&count=250\"\n\nurl_2 &lt;- \"https://m.imdb.com/search/title/?title_type=feature&release_date=,2009-12-31&num_votes=2500,&country_of_origin=TR&count=250\"\n\ncombine_vector &lt;- c(url_1, url_2)\n\n# creating empty vectors\n\ntitles &lt;- c()\nyears &lt;- c()\ndurations &lt;- c()\nratings &lt;- c()\nvotes &lt;- c()\n\n# Scrapping the data\n\nfor (url in combine_vector) {\n  page &lt;- read_html(url)\n  \n  # Extract movie names\n  \n  title_names &lt;- page %&gt;% html_nodes('.ipc-title__text') %&gt;% html_text()\n  title_names &lt;- tail(head(title_names, -1), -1)\n  title_names &lt;- str_split(title_names, \" \", n=2)\n  title_names &lt;- unlist(lapply(title_names, function(x) {x[2]}))\n  \n  \n  # Extract year\n  year &lt;- page %&gt;% html_nodes('.sc-300a8231-7:nth-child(1)') %&gt;% html_text() %&gt;% substr(1, 4) %&gt;% as.numeric()\n  \n  \n  # Extract rating\n  rating &lt;- page %&gt;%\n    html_nodes('.ipc-rating-star--rating') %&gt;%\n    html_text() %&gt;%\n    substr(1, 3) %&gt;%\n    as.numeric()\n  \n  \n  # Extract number of votes\n  vote &lt;- page %&gt;% html_nodes(\".ipc-rating-star--voteCount\") %&gt;% html_text() %&gt;% parse_number()\n  vote &lt;- gsub(\"\\\\(|\\\\)| \", \"\", vote)\n  vote &lt;- as.numeric(vote)\n  \n  \n  # Extract duration\n  duration &lt;- page %&gt;% html_nodes('.sc-300a8231-7:nth-child(2)') %&gt;% html_text()\n  \n  \n  # Extract hour part, if present\n  hour &lt;- str_extract(duration, \"\\\\d+h\") %&gt;%\n    str_replace(\"h\", \"\") %&gt;%\n    as.numeric()\n  \n  \n  # If hour is NA, set it to 0\n  hour[is.na(hour)] &lt;- 0\n  \n  \n  # Extract minute part, if present\n  minute &lt;- str_extract(duration, \"\\\\d+m\") %&gt;%\n    str_replace(\"m\", \"\") %&gt;%\n    as.numeric()\n  \n  \n  # If minute is NA, set it to 0\n  minute[is.na(minute)] &lt;- 0\n  \n  \n  # Calculate total duration\n  total_duration &lt;- (hour * 60) + minute\n  \n  \n  # Append data to vectors\n  titles &lt;- append(titles, title_names)\n  years &lt;- append(years, year)\n  ratings &lt;- append(ratings, rating)\n  votes &lt;- append(votes, vote)\n  durations &lt;- append(durations, total_duration)\n}\n\n\n# Create a data frame from the scraped data\nimdb_data &lt;- data.frame(Title = titles, Year = years, Duration = durations, Rating = ratings, Votes = votes)\n\n\n# Print the first few rows\nprint(head(imdb_data))\n\n\n                     Title Year Duration Rating Votes\n1        Kuru Otlar √úst√ºne 2023      197    7.7    16\n2 Yedinci Kogustaki Mucize 2019      132    8.2    58\n3                   Baskin 2015       97    5.8    13\n4               Kis Uykusu 2014      196    8.0    57\n5        Cebimdeki Yabanci 2018       95    6.8    10\n6                     Ayla 2017      125    8.2    45\n\n\n\nAs seen here, our data set, which we scrapped from the IMDB website, contains the columns Title, Year, Duration, Rating and Votes respectively. Votes values ‚Äã‚Äãhere represent thousands. (16 =&gt; 16K, 4.1 =&gt; 4.1K)\nDue to the problem that arose during the scrapping process, the dataset consists of 50 movies, 25 in the first URL (between 2009 and 2023) and the first 25 in the second URL (before 2009).\nSince the data set does not include all movies, the comments made are based on only the 50 movies selected here for all Turkish movies.",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html#finding-the-highest-and-lowest-rated-movies",
    "href": "assignments/assignment-2.html#finding-the-highest-and-lowest-rated-movies",
    "title": "Assignment 2",
    "section": "Finding the Highest and Lowest Rated Movies",
    "text": "Finding the Highest and Lowest Rated Movies\n\n\nCode\n# Sort by rating in descending order and get the top 5\n\ntop_5 &lt;- imdb_data %&gt;% \n  arrange(desc(Rating)) %&gt;% \n  head(5)\n\nprint(top_5)\n\n\n                     Title Year Duration Rating Votes\n1           Hababam Sinifi 1975       85    9.2    44\n2 Yedinci Kogustaki Mucize 2019      132    8.2    58\n3                     Ayla 2017      125    8.2    45\n4                   Dag II 2016      135    8.2   111\n5           Babam ve Oglum 2005      108    8.2    96\n\n\n\nAs seen here, the top 5 movies with the highest ratings are ‚ÄúHababam Sƒ±nƒ±fƒ±‚Äù, ‚ÄúYedinci Koƒüu≈ütaki Mucize‚Äù, ‚ÄúAyla‚Äù, ‚ÄúDaƒü2‚Äù and ‚ÄúBabam ve Oƒülum‚Äù.\nI have only watched Hababam Sƒ±nƒ±fƒ± out of these movies and I can say that it deserves such a high rating without a doubt.\n\n\n\nCode\n# Sort by rating in ascending order and get the bottom 5\n\nbottom_5 &lt;- imdb_data %&gt;% \n  arrange(Rating) %&gt;% \n  head(5)\n\nprint(bottom_5)\n\n\n                    Title Year Duration Rating Votes\n1                  Bihter 2023      113    3.6   4.1\n2                   D@bbe 2006      110    4.3   4.9\n3            Recep Ivedik 2008       90    4.9  30.0\n4 Istanbul I√ßin Son √áagri 2023       91    5.3   9.6\n5    Kurtlar Vadisi: Irak 2006      122    5.7  19.0\n\n\n\nAs seen here, the top 5 movies with the lowest ratings are ‚ÄúBihter‚Äù, ‚ÄúDabbe‚Äù, ‚ÄúRecep ƒ∞vedik‚Äù, ‚Äúƒ∞stanbul ƒ∞√ßin Son √áaƒürƒ±‚Äù and ‚ÄúKurtlar Vadisi: Irak‚Äù.\nThe fact that I haven‚Äôt watched any of these movies may be a sign that I have good taste in movies. üòÄ",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html#seeing-rating-values-of-selected-movies",
    "href": "assignments/assignment-2.html#seeing-rating-values-of-selected-movies",
    "title": "Assignment 2",
    "section": "Seeing Rating Values ‚Äã‚Äãof Selected Movies",
    "text": "Seeing Rating Values ‚Äã‚Äãof Selected Movies\n\n\nCode\n# Filter the data for the movies \"Aile Arasƒ±nda\" and \"Av Mevsimi\"\nselected_movies &lt;- imdb_data %&gt;% \n  filter(Title %in% c(\"Aile Arasinda\", \"Av Mevsimi\"))\n\n# Print the results\nprint(selected_movies[, c(\"Title\", \"Rating\")])\n\n\n          Title Rating\n1    Av Mevsimi    7.4\n2 Aile Arasinda    7.6\n\n\n\nHere you can see the rating values ‚Äã‚Äãof my favorite movies, ‚ÄúAile Arasƒ±nda‚Äù and ‚ÄúAv Mevsimi‚Äù. I was expecting it to come out between 7 and 8.",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html#yearly-average-movie-ratings",
    "href": "assignments/assignment-2.html#yearly-average-movie-ratings",
    "title": "Assignment 2",
    "section": "Yearly Average Movie Ratings",
    "text": "Yearly Average Movie Ratings\n\n\nCode\n# Calculate the average rating for each year\nyearly_avg &lt;- imdb_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Average_Rating = mean(Rating, na.rm = TRUE))\n\n# Plotting yearly average ratings\nggplot(yearly_avg, aes(x = Year, y = Average_Rating)) +\n  geom_point(color = \"red\", size = 3) +\n  geom_line(color = \"blue\", size = 1) +\n  labs(\n    title = \"Yearly Average Movie Ratings\",\n    x = \"Year\",\n    y = \"Average Rating\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12)\n  )\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\ni Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nThis chart shows the average rating values ‚Äã‚Äãof movies for each year and their changes from year to year. Accordingly, it can be seen that there was a sharp decline in the early 2000s and that it fluctuated after that point. I would like to remind you again that these comments do not reflect the truth, as our data set only includes 50 films.",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html#box-plot-of-movie-ratings-by-year",
    "href": "assignments/assignment-2.html#box-plot-of-movie-ratings-by-year",
    "title": "Assignment 2",
    "section": "Box Plot of Movie Ratings by Year",
    "text": "Box Plot of Movie Ratings by Year\n\n\nCode\n# Create a box plot for yearly movie ratings\nggplot(imdb_data, aes(x = factor(Year), y = Rating)) +\n  geom_boxplot(fill = \"skyblue\", color = \"darkblue\", outlier.color = \"red\") +\n  labs(\n    title = \"Box Plot of Movie Ratings by Year\",\n    x = \"Year\",\n    y = \"Rating\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\nAs can be seen here, the rating values ‚Äã‚Äãof the films in 2006 and 2023 were more widely distributed compared to other years. Since ‚ÄúHababam Sƒ±nƒ±fƒ±‚Äù had the highest rating value in 1975, it was the highest in both this and the previous graph.",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html#correlation-between-votes-and-ratings",
    "href": "assignments/assignment-2.html#correlation-between-votes-and-ratings",
    "title": "Assignment 2",
    "section": "Correlation Between Votes and Ratings",
    "text": "Correlation Between Votes and Ratings\n\n\nCode\n# Calculate correlation between Votes and Rating\ncorrelation &lt;- cor(imdb_data$Votes, imdb_data$Rating, use = \"complete.obs\")\n\n# Print the correlation result\nprint(paste(\"Correlation between Votes and Rating: \", correlation))\n\n\n[1] \"Correlation between Votes and Rating:  0.480028537590679\"\n\n\n\nAs seen here, the correlation between votes and rating is not very clear. We cannot say that a movie that gets a lot of votes will have a high rating value.",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html#correlation-between-durations-and-ratings",
    "href": "assignments/assignment-2.html#correlation-between-durations-and-ratings",
    "title": "Assignment 2",
    "section": "Correlation Between Durations and Ratings",
    "text": "Correlation Between Durations and Ratings\n\n\nCode\n# Calculate correlation between Durations and Rating\ncorrelation &lt;- cor(imdb_data$Duration, imdb_data$Rating, use = \"complete.obs\")\n\n# Print the correlation result\nprint(paste(\"Correlation between Durations and Rating: \", correlation))\n\n\n[1] \"Correlation between Durations and Rating:  0.225018765415274\"\n\n\n\nHere, the correlation between duration and rating is much lower. There is almost no relationship between them.",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html#imdb-top-1000-turkish-movies",
    "href": "assignments/assignment-2.html#imdb-top-1000-turkish-movies",
    "title": "Assignment 2",
    "section": "IMDB Top 1000 Turkish Movies",
    "text": "IMDB Top 1000 Turkish Movies\n\n\nCode\n# Scrapping Turkish Movies in top 1000\n\nurl_new &lt;- \"https://m.imdb.com/search/title/?title_type=feature&groups=top_1000&country_of_origin=TR\"\n\n# creating empty vectors\n\ntitles &lt;- c()\nyears &lt;- c()\ndurations &lt;- c()\nratings &lt;- c()\nvotes &lt;- c()\n\nfor(url in url_new){\n  page = read_html(url)\n\n# Extract movie names\n\ntitle_names &lt;- page %&gt;% html_nodes('.ipc-title__text') %&gt;% html_text()\ntitle_names &lt;- tail(head(title_names, -1), -1)\ntitle_names &lt;- str_split(title_names, \" \", n=2)\ntitle_names &lt;- unlist(lapply(title_names, function(x) {x[2]}))\n\n\n# Extract year\nyear &lt;- page %&gt;% html_nodes('.sc-300a8231-7:nth-child(1)') %&gt;% html_text() %&gt;% substr(1, 4) %&gt;% as.numeric()\n\n\n# Extract rating\nrating &lt;- page %&gt;%\n  html_nodes('.ipc-rating-star--rating') %&gt;%\n  html_text() %&gt;%\n  substr(1, 3) %&gt;%\n  as.numeric()\n\n\n# Extract number of votes\nvote &lt;- page %&gt;% html_nodes(\".ipc-rating-star--voteCount\") %&gt;% html_text() %&gt;% parse_number()\nvote &lt;- gsub(\"\\\\(|\\\\)| \", \"\", vote)\nvote &lt;- as.numeric(vote)\n\n\n# Extract duration\nduration &lt;- page %&gt;% html_nodes('.sc-300a8231-7:nth-child(2)') %&gt;% html_text()\n\n\n# Extract hour part, if present\nhour &lt;- str_extract(duration, \"\\\\d+h\") %&gt;%\n  str_replace(\"h\", \"\") %&gt;%\n  as.numeric()\n\n\n# If hour is NA, set it to 0\nhour[is.na(hour)] &lt;- 0\n\n\n# Extract minute part, if present\nminute &lt;- str_extract(duration, \"\\\\d+m\") %&gt;%\n  str_replace(\"m\", \"\") %&gt;%\n  as.numeric()\n\n\n# If minute is NA, set it to 0\nminute[is.na(minute)] &lt;- 0\n\n\n# Calculate total duration\ntotal_duration &lt;- (hour * 60) + minute\n\n\n# Append data to vectors\ntitles &lt;- append(titles, title_names)\nyears &lt;- append(years, year)\nratings &lt;- append(ratings, rating)\nvotes &lt;- append(votes, vote)\ndurations &lt;- append(durations, total_duration)\n\n}\n\n# Create a data frame from the scraped data\ntop1000_turkish &lt;- data.frame(Title = titles, Year = years, Duration = durations, Rating = ratings, Votes = votes)\n\n\n# Print the dataframe \nprint(top1000_turkish[1:2])\n\n\n                      Title Year\n1  Yedinci Kogustaki Mucize 2019\n2                Kis Uykusu 2014\n3                      Ayla 2017\n4   Bir Zamanlar Anadolu'da 2011\n5            Babam ve Oglum 2005\n6               Ahlat Agaci 2018\n7                  G.O.R.A. 2004\n8                    Eskiya 1996\n9                     Nefes 2009\n10                Vizontele 2001\n11 Her Sey √áok G√ºzel Olacak 1998\n\n\n\nHere we see Turkish movies that are among the top 1000 movies on IMDB. There are 11 movies in total.\n\n\n\nCode\n# Sorting from largest to smallest according to rating value\ntop1000_turkish_sorted &lt;- top1000_turkish[order(-top1000_turkish$Rating), ]\n\nprint(top1000_turkish_sorted)\n\n\n                      Title Year Duration Rating Votes\n1  Yedinci Kogustaki Mucize 2019      132    8.2    58\n3                      Ayla 2017      125    8.2    45\n5            Babam ve Oglum 2005      108    8.2    96\n8                    Eskiya 1996      128    8.1    73\n11 Her Sey √áok G√ºzel Olacak 1998      107    8.1    29\n2                Kis Uykusu 2014      196    8.0    57\n6               Ahlat Agaci 2018      188    8.0    29\n7                  G.O.R.A. 2004      127    8.0    69\n9                     Nefes 2009      128    8.0    36\n10                Vizontele 2001      110    8.0    40\n4   Bir Zamanlar Anadolu'da 2011      157    7.8    52\n\n\n\nHere is a list of these 11 movies ranked by rating.\nThis ranking does not match the ranking I made before. For example, Hababam Sƒ±nƒ±fƒ± is not even on the list. For this reason, I think they may have considered other categories besides rating.\n\n.\n.\n.\nWe have come to the end of our EMU430 Course\n.\n.\n.\nTake care of yourself\n.\n.\n.\nDon‚Äôt miss data from your life\n.\n.\n.",
    "crumbs": [
      "Assignment 2"
    ]
  }
]